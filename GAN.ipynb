{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0000 D loss: -0.3867 G loss: -2.184\n",
      "Epoch: 0001 D loss: -0.1678 G loss: -2.719\n",
      "Epoch: 0002 D loss: -0.121 G loss: -3.212\n",
      "Epoch: 0003 D loss: -0.3947 G loss: -1.955\n",
      "Epoch: 0004 D loss: -0.3976 G loss: -1.836\n",
      "Epoch: 0005 D loss: -0.1434 G loss: -3.168\n",
      "Epoch: 0006 D loss: -0.1542 G loss: -3.879\n",
      "Epoch: 0007 D loss: -0.2439 G loss: -2.549\n",
      "Epoch: 0008 D loss: -0.2214 G loss: -2.562\n",
      "Epoch: 0009 D loss: -0.1888 G loss: -3.015\n",
      "Epoch: 0010 D loss: -0.2253 G loss: -2.816\n",
      "Epoch: 0011 D loss: -0.3923 G loss: -2.345\n",
      "Epoch: 0012 D loss: -0.2735 G loss: -2.937\n",
      "Epoch: 0013 D loss: -0.6661 G loss: -1.933\n",
      "Epoch: 0014 D loss: -0.4049 G loss: -2.583\n",
      "Epoch: 0015 D loss: -0.5525 G loss: -2.337\n",
      "Epoch: 0016 D loss: -0.5136 G loss: -2.169\n",
      "Epoch: 0017 D loss: -0.2494 G loss: -2.797\n",
      "Epoch: 0018 D loss: -0.3859 G loss: -2.598\n",
      "Epoch: 0019 D loss: -0.4765 G loss: -2.69\n",
      "Epoch: 0020 D loss: -0.4288 G loss: -2.564\n",
      "Epoch: 0021 D loss: -0.3846 G loss: -2.321\n",
      "Epoch: 0022 D loss: -0.3998 G loss: -2.421\n",
      "Epoch: 0023 D loss: -0.3421 G loss: -2.481\n",
      "Epoch: 0024 D loss: -0.4374 G loss: -2.603\n",
      "Epoch: 0025 D loss: -0.4365 G loss: -2.632\n",
      "Epoch: 0026 D loss: -0.3986 G loss: -2.318\n",
      "Epoch: 0027 D loss: -0.4583 G loss: -2.5\n",
      "Epoch: 0028 D loss: -0.3566 G loss: -2.624\n",
      "Epoch: 0029 D loss: -0.5591 G loss: -2.763\n",
      "Epoch: 0030 D loss: -0.5688 G loss: -2.194\n",
      "Epoch: 0031 D loss: -0.6281 G loss: -2.441\n",
      "Epoch: 0032 D loss: -0.5459 G loss: -2.183\n",
      "Epoch: 0033 D loss: -0.7836 G loss: -1.857\n",
      "Epoch: 0034 D loss: -0.7424 G loss: -2.08\n",
      "Epoch: 0035 D loss: -0.6034 G loss: -2.149\n",
      "Epoch: 0036 D loss: -0.5263 G loss: -2.563\n",
      "Epoch: 0037 D loss: -0.5643 G loss: -2.359\n",
      "Epoch: 0038 D loss: -0.5835 G loss: -2.132\n",
      "Epoch: 0039 D loss: -0.7879 G loss: -2.075\n",
      "Epoch: 0040 D loss: -0.778 G loss: -2.32\n",
      "Epoch: 0041 D loss: -0.7337 G loss: -1.715\n",
      "Epoch: 0042 D loss: -0.5663 G loss: -2.177\n",
      "Epoch: 0043 D loss: -0.7626 G loss: -2.161\n",
      "Epoch: 0044 D loss: -0.982 G loss: -1.664\n",
      "Epoch: 0045 D loss: -0.9058 G loss: -1.779\n",
      "Epoch: 0046 D loss: -0.9396 G loss: -1.76\n",
      "Epoch: 0047 D loss: -0.9036 G loss: -1.628\n",
      "Epoch: 0048 D loss: -0.8982 G loss: -1.465\n",
      "Epoch: 0049 D loss: -0.838 G loss: -1.564\n",
      "Epoch: 0050 D loss: -0.9531 G loss: -1.543\n",
      "Epoch: 0051 D loss: -0.8835 G loss: -1.526\n",
      "Epoch: 0052 D loss: -0.7504 G loss: -1.539\n",
      "Epoch: 0053 D loss: -0.916 G loss: -1.64\n",
      "Epoch: 0054 D loss: -0.7872 G loss: -1.633\n",
      "Epoch: 0055 D loss: -0.931 G loss: -1.671\n",
      "Epoch: 0056 D loss: -0.8622 G loss: -1.505\n",
      "Epoch: 0057 D loss: -0.9498 G loss: -1.579\n",
      "Epoch: 0058 D loss: -0.9528 G loss: -1.784\n",
      "Epoch: 0059 D loss: -0.995 G loss: -1.68\n",
      "Epoch: 0060 D loss: -0.8141 G loss: -1.624\n",
      "Epoch: 0061 D loss: -0.9305 G loss: -1.624\n",
      "Epoch: 0062 D loss: -0.9707 G loss: -1.527\n",
      "Epoch: 0063 D loss: -0.8644 G loss: -1.584\n",
      "Epoch: 0064 D loss: -0.892 G loss: -1.487\n",
      "Epoch: 0065 D loss: -0.8732 G loss: -1.719\n",
      "Epoch: 0066 D loss: -0.946 G loss: -1.526\n",
      "Epoch: 0067 D loss: -0.9138 G loss: -1.549\n",
      "Epoch: 0068 D loss: -0.9416 G loss: -1.595\n",
      "Epoch: 0069 D loss: -0.9653 G loss: -1.651\n",
      "Epoch: 0070 D loss: -0.8174 G loss: -1.559\n",
      "Epoch: 0071 D loss: -0.9068 G loss: -1.54\n",
      "Epoch: 0072 D loss: -0.9643 G loss: -1.481\n",
      "Epoch: 0073 D loss: -1.041 G loss: -1.456\n",
      "Epoch: 0074 D loss: -0.9191 G loss: -1.629\n",
      "Epoch: 0075 D loss: -0.9246 G loss: -1.629\n",
      "Epoch: 0076 D loss: -0.9344 G loss: -1.461\n",
      "Epoch: 0077 D loss: -0.9108 G loss: -1.57\n",
      "Epoch: 0078 D loss: -0.9424 G loss: -1.568\n",
      "Epoch: 0079 D loss: -0.9722 G loss: -1.359\n",
      "Epoch: 0080 D loss: -0.9068 G loss: -1.428\n",
      "Epoch: 0081 D loss: -0.9474 G loss: -1.463\n",
      "Epoch: 0082 D loss: -0.9214 G loss: -1.574\n",
      "Epoch: 0083 D loss: -0.9455 G loss: -1.523\n",
      "Epoch: 0084 D loss: -1.045 G loss: -1.573\n",
      "Epoch: 0085 D loss: -0.973 G loss: -1.684\n",
      "Epoch: 0086 D loss: -0.9018 G loss: -1.462\n",
      "Epoch: 0087 D loss: -1.093 G loss: -1.562\n",
      "Epoch: 0088 D loss: -0.8777 G loss: -1.662\n",
      "Epoch: 0089 D loss: -0.625 G loss: -1.656\n",
      "Epoch: 0090 D loss: -0.9089 G loss: -1.519\n",
      "Epoch: 0091 D loss: -0.8767 G loss: -1.565\n",
      "Epoch: 0092 D loss: -0.8591 G loss: -1.432\n",
      "Epoch: 0093 D loss: -0.8434 G loss: -1.641\n",
      "Epoch: 0094 D loss: -1.025 G loss: -1.421\n",
      "Epoch: 0095 D loss: -0.8326 G loss: -1.65\n",
      "Epoch: 0096 D loss: -0.7958 G loss: -1.516\n",
      "Epoch: 0097 D loss: -0.9558 G loss: -1.622\n",
      "Epoch: 0098 D loss: -0.9353 G loss: -1.561\n",
      "Epoch: 0099 D loss: -0.8703 G loss: -1.572\n",
      "Epoch: 0100 D loss: -0.978 G loss: -1.381\n",
      "Epoch: 0101 D loss: -0.9157 G loss: -1.476\n",
      "Epoch: 0102 D loss: -0.9041 G loss: -1.545\n",
      "Epoch: 0103 D loss: -0.9775 G loss: -1.556\n",
      "Epoch: 0104 D loss: -0.8976 G loss: -1.559\n",
      "Epoch: 0105 D loss: -0.8867 G loss: -1.669\n",
      "Epoch: 0106 D loss: -0.8898 G loss: -1.795\n",
      "Epoch: 0107 D loss: -0.9306 G loss: -1.573\n",
      "Epoch: 0108 D loss: -0.9703 G loss: -1.513\n",
      "Epoch: 0109 D loss: -0.9485 G loss: -1.507\n",
      "Epoch: 0110 D loss: -0.868 G loss: -1.611\n",
      "Epoch: 0111 D loss: -0.9222 G loss: -1.617\n",
      "Epoch: 0112 D loss: -0.7645 G loss: -1.629\n",
      "Epoch: 0113 D loss: -0.7579 G loss: -1.897\n",
      "Epoch: 0114 D loss: -0.801 G loss: -1.781\n",
      "Epoch: 0115 D loss: -0.9883 G loss: -1.614\n",
      "Epoch: 0116 D loss: -0.9046 G loss: -1.542\n",
      "Epoch: 0117 D loss: -0.8228 G loss: -1.776\n",
      "Epoch: 0118 D loss: -0.8918 G loss: -1.507\n",
      "Epoch: 0119 D loss: -0.8429 G loss: -1.543\n",
      "Epoch: 0120 D loss: -0.8478 G loss: -1.542\n",
      "Epoch: 0121 D loss: -0.7949 G loss: -1.775\n",
      "Epoch: 0122 D loss: -0.7097 G loss: -1.526\n",
      "Epoch: 0123 D loss: -0.785 G loss: -1.67\n",
      "Epoch: 0124 D loss: -0.7419 G loss: -1.887\n",
      "Epoch: 0125 D loss: -0.7635 G loss: -1.813\n",
      "Epoch: 0126 D loss: -0.8225 G loss: -1.744\n",
      "Epoch: 0127 D loss: -0.9461 G loss: -1.699\n",
      "Epoch: 0128 D loss: -0.7527 G loss: -1.857\n",
      "Epoch: 0129 D loss: -0.7348 G loss: -1.916\n",
      "Epoch: 0130 D loss: -0.8854 G loss: -1.731\n",
      "Epoch: 0131 D loss: -0.6935 G loss: -1.96\n",
      "Epoch: 0132 D loss: -0.8669 G loss: -1.688\n",
      "Epoch: 0133 D loss: -0.7965 G loss: -1.994\n",
      "Epoch: 0134 D loss: -0.8312 G loss: -1.621\n",
      "Epoch: 0135 D loss: -0.7522 G loss: -1.938\n",
      "Epoch: 0136 D loss: -0.7897 G loss: -1.95\n",
      "Epoch: 0137 D loss: -0.6776 G loss: -2.03\n",
      "Epoch: 0138 D loss: -0.8238 G loss: -1.717\n",
      "Epoch: 0139 D loss: -0.7874 G loss: -1.805\n",
      "Epoch: 0140 D loss: -0.7808 G loss: -1.788\n",
      "Epoch: 0141 D loss: -0.7944 G loss: -1.823\n",
      "Epoch: 0142 D loss: -0.7997 G loss: -1.975\n",
      "Epoch: 0143 D loss: -0.8046 G loss: -1.799\n",
      "Epoch: 0144 D loss: -0.6643 G loss: -2.015\n",
      "Epoch: 0145 D loss: -0.8121 G loss: -1.627\n",
      "Epoch: 0146 D loss: -0.6436 G loss: -1.893\n",
      "Epoch: 0147 D loss: -0.7935 G loss: -1.609\n",
      "Epoch: 0148 D loss: -0.7339 G loss: -1.654\n",
      "Epoch: 0149 D loss: -0.7964 G loss: -1.781\n",
      "Epoch: 0150 D loss: -0.7233 G loss: -1.83\n",
      "Epoch: 0151 D loss: -0.7877 G loss: -1.866\n",
      "Epoch: 0152 D loss: -0.7223 G loss: -1.874\n",
      "Epoch: 0153 D loss: -0.6753 G loss: -2.037\n",
      "Epoch: 0154 D loss: -0.6953 G loss: -1.872\n",
      "Epoch: 0155 D loss: -0.6924 G loss: -1.95\n",
      "Epoch: 0156 D loss: -0.6272 G loss: -1.925\n",
      "Epoch: 0157 D loss: -0.5364 G loss: -2.191\n",
      "Epoch: 0158 D loss: -0.7361 G loss: -2.121\n",
      "Epoch: 0159 D loss: -0.6027 G loss: -1.959\n",
      "Epoch: 0160 D loss: -0.7477 G loss: -1.858\n",
      "Epoch: 0161 D loss: -0.6626 G loss: -2.002\n",
      "Epoch: 0162 D loss: -0.7003 G loss: -2.103\n",
      "Epoch: 0163 D loss: -0.7464 G loss: -2.121\n",
      "Epoch: 0164 D loss: -0.642 G loss: -2.013\n",
      "Epoch: 0165 D loss: -0.7643 G loss: -2.005\n",
      "Epoch: 0166 D loss: -0.5903 G loss: -2.289\n",
      "Epoch: 0167 D loss: -0.7235 G loss: -1.973\n",
      "Epoch: 0168 D loss: -0.6754 G loss: -2.173\n",
      "Epoch: 0169 D loss: -0.6896 G loss: -2.269\n",
      "Epoch: 0170 D loss: -0.7148 G loss: -2.059\n",
      "Epoch: 0171 D loss: -0.7692 G loss: -1.894\n",
      "Epoch: 0172 D loss: -0.7456 G loss: -1.815\n",
      "Epoch: 0173 D loss: -0.589 G loss: -2.225\n",
      "Epoch: 0174 D loss: -0.5809 G loss: -2.333\n",
      "Epoch: 0175 D loss: -0.6579 G loss: -2.198\n",
      "Epoch: 0176 D loss: -0.7396 G loss: -2.178\n",
      "Epoch: 0177 D loss: -0.6195 G loss: -2.198\n",
      "Epoch: 0178 D loss: -0.585 G loss: -2.125\n",
      "Epoch: 0179 D loss: -0.7668 G loss: -2.214\n",
      "Epoch: 0180 D loss: -0.6577 G loss: -2.429\n",
      "Epoch: 0181 D loss: -0.541 G loss: -2.451\n",
      "Epoch: 0182 D loss: -0.5239 G loss: -2.054\n",
      "Epoch: 0183 D loss: -0.6174 G loss: -2.276\n",
      "Epoch: 0184 D loss: -0.6017 G loss: -2.591\n",
      "Epoch: 0185 D loss: -0.6434 G loss: -2.326\n",
      "Epoch: 0186 D loss: -0.6678 G loss: -2.306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0187 D loss: -0.6169 G loss: -2.402\n",
      "Epoch: 0188 D loss: -0.4791 G loss: -2.311\n",
      "Epoch: 0189 D loss: -0.5666 G loss: -2.327\n",
      "Epoch: 0190 D loss: -0.4579 G loss: -2.63\n",
      "Epoch: 0191 D loss: -0.6296 G loss: -2.581\n",
      "Epoch: 0192 D loss: -0.694 G loss: -2.637\n",
      "Epoch: 0193 D loss: -0.5238 G loss: -2.329\n",
      "Epoch: 0194 D loss: -0.5925 G loss: -2.161\n",
      "Epoch: 0195 D loss: -0.5382 G loss: -2.31\n",
      "Epoch: 0196 D loss: -0.5408 G loss: -2.456\n",
      "Epoch: 0197 D loss: -0.4758 G loss: -2.604\n",
      "Epoch: 0198 D loss: -0.6131 G loss: -2.133\n",
      "Epoch: 0199 D loss: -0.5018 G loss: -2.66\n",
      "Epoch: 0200 D loss: -0.5653 G loss: -2.677\n",
      "Epoch: 0201 D loss: -0.7049 G loss: -2.155\n",
      "Epoch: 0202 D loss: -0.5597 G loss: -2.45\n",
      "Epoch: 0203 D loss: -0.5231 G loss: -2.425\n",
      "Epoch: 0204 D loss: -0.6064 G loss: -2.242\n",
      "Epoch: 0205 D loss: -0.5021 G loss: -2.475\n",
      "Epoch: 0206 D loss: -0.591 G loss: -2.511\n",
      "Epoch: 0207 D loss: -0.4817 G loss: -2.464\n",
      "Epoch: 0208 D loss: -0.4895 G loss: -2.47\n",
      "Epoch: 0209 D loss: -0.4634 G loss: -2.573\n",
      "Epoch: 0210 D loss: -0.6113 G loss: -2.529\n",
      "Epoch: 0211 D loss: -0.5548 G loss: -2.599\n",
      "Epoch: 0212 D loss: -0.5066 G loss: -2.6\n",
      "Epoch: 0213 D loss: -0.4699 G loss: -2.722\n",
      "Epoch: 0214 D loss: -0.4308 G loss: -2.589\n",
      "Epoch: 0215 D loss: -0.5234 G loss: -2.99\n",
      "Epoch: 0216 D loss: -0.5101 G loss: -2.567\n",
      "Epoch: 0217 D loss: -0.388 G loss: -2.657\n",
      "Epoch: 0218 D loss: -0.549 G loss: -2.466\n",
      "Epoch: 0219 D loss: -0.4508 G loss: -2.877\n",
      "Epoch: 0220 D loss: -0.4444 G loss: -2.811\n",
      "Epoch: 0221 D loss: -0.5925 G loss: -2.706\n",
      "Epoch: 0222 D loss: -0.4738 G loss: -2.819\n",
      "Epoch: 0223 D loss: -0.4747 G loss: -2.682\n",
      "Epoch: 0224 D loss: -0.5252 G loss: -2.823\n",
      "Epoch: 0225 D loss: -0.4165 G loss: -2.745\n",
      "Epoch: 0226 D loss: -0.5233 G loss: -2.352\n",
      "Epoch: 0227 D loss: -0.557 G loss: -3.042\n",
      "Epoch: 0228 D loss: -0.4891 G loss: -2.329\n",
      "Epoch: 0229 D loss: -0.489 G loss: -2.476\n",
      "Epoch: 0230 D loss: -0.7062 G loss: -2.693\n",
      "Epoch: 0231 D loss: -0.5178 G loss: -2.687\n",
      "Epoch: 0232 D loss: -0.555 G loss: -2.708\n",
      "Epoch: 0233 D loss: -0.6448 G loss: -2.964\n",
      "Epoch: 0234 D loss: -0.5489 G loss: -3.079\n",
      "Epoch: 0235 D loss: -0.5622 G loss: -2.621\n",
      "Epoch: 0236 D loss: -0.4202 G loss: -2.816\n",
      "Epoch: 0237 D loss: -0.4962 G loss: -2.706\n",
      "Epoch: 0238 D loss: -0.531 G loss: -2.634\n",
      "Epoch: 0239 D loss: -0.5317 G loss: -3.073\n",
      "Epoch: 0240 D loss: -0.3785 G loss: -2.693\n",
      "Epoch: 0241 D loss: -0.4897 G loss: -2.341\n",
      "Epoch: 0242 D loss: -0.4573 G loss: -2.762\n",
      "Epoch: 0243 D loss: -0.4415 G loss: -2.642\n",
      "Epoch: 0244 D loss: -0.4946 G loss: -2.611\n",
      "Epoch: 0245 D loss: -0.5614 G loss: -2.89\n",
      "Epoch: 0246 D loss: -0.5221 G loss: -2.61\n",
      "Epoch: 0247 D loss: -0.4713 G loss: -2.569\n",
      "Epoch: 0248 D loss: -0.4451 G loss: -2.633\n",
      "Epoch: 0249 D loss: -0.4935 G loss: -2.877\n",
      "Epoch: 0250 D loss: -0.5467 G loss: -3.102\n",
      "Epoch: 0251 D loss: -0.443 G loss: -2.968\n",
      "Epoch: 0252 D loss: -0.6193 G loss: -2.799\n",
      "Epoch: 0253 D loss: -0.5241 G loss: -2.788\n",
      "Epoch: 0254 D loss: -0.409 G loss: -2.851\n",
      "Epoch: 0255 D loss: -0.6922 G loss: -2.41\n",
      "Epoch: 0256 D loss: -0.4049 G loss: -2.984\n",
      "Epoch: 0257 D loss: -0.4252 G loss: -2.873\n",
      "Epoch: 0258 D loss: -0.4562 G loss: -3.034\n",
      "Epoch: 0259 D loss: -0.4789 G loss: -2.827\n",
      "Epoch: 0260 D loss: -0.4638 G loss: -3.379\n",
      "Epoch: 0261 D loss: -0.577 G loss: -2.789\n",
      "Epoch: 0262 D loss: -0.4254 G loss: -3.085\n",
      "Epoch: 0263 D loss: -0.4701 G loss: -2.946\n",
      "Epoch: 0264 D loss: -0.4333 G loss: -2.751\n",
      "Epoch: 0265 D loss: -0.6214 G loss: -3.205\n",
      "Epoch: 0266 D loss: -0.513 G loss: -2.732\n",
      "Epoch: 0267 D loss: -0.4577 G loss: -2.982\n",
      "Epoch: 0268 D loss: -0.579 G loss: -2.96\n",
      "Epoch: 0269 D loss: -0.5543 G loss: -2.877\n",
      "Epoch: 0270 D loss: -0.3875 G loss: -2.935\n",
      "Epoch: 0271 D loss: -0.5414 G loss: -2.903\n",
      "Epoch: 0272 D loss: -0.3974 G loss: -2.887\n",
      "Epoch: 0273 D loss: -0.4458 G loss: -3.193\n",
      "Epoch: 0274 D loss: -0.5291 G loss: -2.777\n",
      "Epoch: 0275 D loss: -0.4619 G loss: -2.88\n",
      "Epoch: 0276 D loss: -0.3918 G loss: -3.05\n",
      "Epoch: 0277 D loss: -0.3308 G loss: -3.041\n",
      "Epoch: 0278 D loss: -0.4881 G loss: -2.991\n",
      "Epoch: 0279 D loss: -0.4857 G loss: -3.021\n",
      "Epoch: 0280 D loss: -0.4434 G loss: -2.641\n",
      "Epoch: 0281 D loss: -0.4959 G loss: -2.821\n",
      "Epoch: 0282 D loss: -0.4351 G loss: -3.008\n",
      "Epoch: 0283 D loss: -0.4423 G loss: -2.876\n",
      "Epoch: 0284 D loss: -0.5311 G loss: -2.813\n",
      "Epoch: 0285 D loss: -0.4617 G loss: -2.588\n",
      "Epoch: 0286 D loss: -0.4502 G loss: -2.871\n",
      "Epoch: 0287 D loss: -0.4172 G loss: -2.988\n",
      "Epoch: 0288 D loss: -0.5104 G loss: -2.425\n",
      "Epoch: 0289 D loss: -0.5974 G loss: -3.17\n",
      "Epoch: 0290 D loss: -0.5051 G loss: -3.025\n",
      "Epoch: 0291 D loss: -0.4108 G loss: -2.758\n",
      "Epoch: 0292 D loss: -0.4394 G loss: -3.083\n",
      "Epoch: 0293 D loss: -0.4246 G loss: -2.942\n",
      "Epoch: 0294 D loss: -0.4371 G loss: -3.189\n",
      "Epoch: 0295 D loss: -0.4913 G loss: -3.115\n",
      "Epoch: 0296 D loss: -0.4054 G loss: -3.145\n",
      "Epoch: 0297 D loss: -0.4563 G loss: -3.233\n",
      "Epoch: 0298 D loss: -0.4312 G loss: -2.728\n",
      "Epoch: 0299 D loss: -0.5446 G loss: -2.826\n",
      "Epoch: 0300 D loss: -0.5109 G loss: -3.029\n",
      "Epoch: 0301 D loss: -0.6247 G loss: -3.05\n",
      "Epoch: 0302 D loss: -0.4272 G loss: -3.334\n",
      "Epoch: 0303 D loss: -0.4017 G loss: -3.106\n",
      "Epoch: 0304 D loss: -0.3871 G loss: -2.986\n",
      "Epoch: 0305 D loss: -0.4159 G loss: -2.816\n",
      "Epoch: 0306 D loss: -0.4841 G loss: -2.597\n",
      "Epoch: 0307 D loss: -0.318 G loss: -3.233\n",
      "Epoch: 0308 D loss: -0.3804 G loss: -3.241\n",
      "Epoch: 0309 D loss: -0.413 G loss: -2.767\n",
      "Epoch: 0310 D loss: -0.344 G loss: -3.252\n",
      "Epoch: 0311 D loss: -0.3624 G loss: -3.271\n",
      "Epoch: 0312 D loss: -0.4386 G loss: -2.931\n",
      "Epoch: 0313 D loss: -0.4094 G loss: -2.84\n",
      "Epoch: 0314 D loss: -0.4403 G loss: -2.99\n",
      "Epoch: 0315 D loss: -0.4383 G loss: -3.105\n",
      "Epoch: 0316 D loss: -0.4156 G loss: -3.005\n",
      "Epoch: 0317 D loss: -0.4769 G loss: -3.011\n",
      "Epoch: 0318 D loss: -0.4197 G loss: -2.982\n",
      "Epoch: 0319 D loss: -0.3918 G loss: -2.874\n",
      "Epoch: 0320 D loss: -0.3791 G loss: -2.791\n",
      "Epoch: 0321 D loss: -0.4022 G loss: -2.878\n",
      "Epoch: 0322 D loss: -0.5153 G loss: -3.157\n",
      "Epoch: 0323 D loss: -0.3638 G loss: -3.271\n",
      "Epoch: 0324 D loss: -0.3384 G loss: -2.523\n",
      "Epoch: 0325 D loss: -0.4084 G loss: -2.974\n",
      "Epoch: 0326 D loss: -0.4068 G loss: -3.193\n",
      "Epoch: 0327 D loss: -0.5297 G loss: -3.388\n",
      "Epoch: 0328 D loss: -0.4327 G loss: -2.898\n",
      "Epoch: 0329 D loss: -0.3852 G loss: -2.91\n",
      "Epoch: 0330 D loss: -0.3536 G loss: -3.38\n",
      "Epoch: 0331 D loss: -0.5177 G loss: -2.571\n",
      "Epoch: 0332 D loss: -0.3697 G loss: -3.558\n",
      "Epoch: 0333 D loss: -0.3808 G loss: -3.163\n",
      "Epoch: 0334 D loss: -0.384 G loss: -3.314\n",
      "Epoch: 0335 D loss: -0.4645 G loss: -3.166\n",
      "Epoch: 0336 D loss: -0.4452 G loss: -3.09\n",
      "Epoch: 0337 D loss: -0.3884 G loss: -3.145\n",
      "Epoch: 0338 D loss: -0.4133 G loss: -3.337\n",
      "Epoch: 0339 D loss: -0.3185 G loss: -2.91\n",
      "Epoch: 0340 D loss: -0.4205 G loss: -3.103\n",
      "Epoch: 0341 D loss: -0.512 G loss: -3.006\n",
      "Epoch: 0342 D loss: -0.3617 G loss: -3.593\n",
      "Epoch: 0343 D loss: -0.4273 G loss: -2.941\n",
      "Epoch: 0344 D loss: -0.41 G loss: -3.537\n",
      "Epoch: 0345 D loss: -0.4471 G loss: -3.106\n",
      "Epoch: 0346 D loss: -0.4716 G loss: -3.387\n",
      "Epoch: 0347 D loss: -0.4973 G loss: -3.165\n",
      "Epoch: 0348 D loss: -0.4487 G loss: -2.74\n",
      "Epoch: 0349 D loss: -0.308 G loss: -3.212\n",
      "Epoch: 0350 D loss: -0.4165 G loss: -3.192\n",
      "Epoch: 0351 D loss: -0.4135 G loss: -3.616\n",
      "Epoch: 0352 D loss: -0.3935 G loss: -3.051\n",
      "Epoch: 0353 D loss: -0.3825 G loss: -3.294\n",
      "Epoch: 0354 D loss: -0.4151 G loss: -3.038\n",
      "Epoch: 0355 D loss: -0.4253 G loss: -3.327\n",
      "Epoch: 0356 D loss: -0.3473 G loss: -3.465\n",
      "Epoch: 0357 D loss: -0.3644 G loss: -3.276\n",
      "Epoch: 0358 D loss: -0.427 G loss: -3.306\n",
      "Epoch: 0359 D loss: -0.3939 G loss: -3.123\n",
      "Epoch: 0360 D loss: -0.431 G loss: -3.017\n",
      "Epoch: 0361 D loss: -0.4657 G loss: -2.94\n",
      "Epoch: 0362 D loss: -0.3466 G loss: -2.87\n",
      "Epoch: 0363 D loss: -0.3676 G loss: -3.135\n",
      "Epoch: 0364 D loss: -0.3596 G loss: -2.936\n",
      "Epoch: 0365 D loss: -0.3762 G loss: -3.304\n",
      "Epoch: 0366 D loss: -0.3286 G loss: -3.627\n",
      "Epoch: 0367 D loss: -0.3985 G loss: -3.171\n",
      "Epoch: 0368 D loss: -0.4244 G loss: -3.156\n",
      "Epoch: 0369 D loss: -0.4582 G loss: -3.22\n",
      "Epoch: 0370 D loss: -0.426 G loss: -3.208\n",
      "Epoch: 0371 D loss: -0.4881 G loss: -3.063\n",
      "Epoch: 0372 D loss: -0.389 G loss: -3.036\n",
      "Epoch: 0373 D loss: -0.4575 G loss: -3.318\n",
      "Epoch: 0374 D loss: -0.4064 G loss: -3.203\n",
      "Epoch: 0375 D loss: -0.3308 G loss: -3.244\n",
      "Epoch: 0376 D loss: -0.4525 G loss: -3.537\n",
      "Epoch: 0377 D loss: -0.5454 G loss: -2.944\n",
      "Epoch: 0378 D loss: -0.4061 G loss: -3.035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0379 D loss: -0.3898 G loss: -3.258\n",
      "Epoch: 0380 D loss: -0.4204 G loss: -3.177\n",
      "Epoch: 0381 D loss: -0.4613 G loss: -3.235\n",
      "Epoch: 0382 D loss: -0.3316 G loss: -3.318\n",
      "Epoch: 0383 D loss: -0.4275 G loss: -3.052\n",
      "Epoch: 0384 D loss: -0.4767 G loss: -3.446\n",
      "Epoch: 0385 D loss: -0.4747 G loss: -3.244\n",
      "Epoch: 0386 D loss: -0.3513 G loss: -3.109\n",
      "Epoch: 0387 D loss: -0.4831 G loss: -3.239\n",
      "Epoch: 0388 D loss: -0.4743 G loss: -3.455\n",
      "Epoch: 0389 D loss: -0.3468 G loss: -3.326\n",
      "Epoch: 0390 D loss: -0.3524 G loss: -3.251\n",
      "Epoch: 0391 D loss: -0.3099 G loss: -3.041\n",
      "Epoch: 0392 D loss: -0.4154 G loss: -3.514\n",
      "Epoch: 0393 D loss: -0.4683 G loss: -3.139\n",
      "Epoch: 0394 D loss: -0.4617 G loss: -3.088\n",
      "Epoch: 0395 D loss: -0.3976 G loss: -3.385\n",
      "Epoch: 0396 D loss: -0.4614 G loss: -3.145\n",
      "Epoch: 0397 D loss: -0.4556 G loss: -3.533\n",
      "Epoch: 0398 D loss: -0.5072 G loss: -3.526\n",
      "Epoch: 0399 D loss: -0.355 G loss: -3.113\n",
      "Epoch: 0400 D loss: -0.3923 G loss: -3.125\n",
      "Epoch: 0401 D loss: -0.3918 G loss: -3.298\n",
      "Epoch: 0402 D loss: -0.4977 G loss: -3.457\n",
      "Epoch: 0403 D loss: -0.4713 G loss: -3.525\n",
      "Epoch: 0404 D loss: -0.3906 G loss: -3.885\n",
      "Epoch: 0405 D loss: -0.3449 G loss: -3.268\n",
      "Epoch: 0406 D loss: -0.3145 G loss: -3.596\n",
      "Epoch: 0407 D loss: -0.4018 G loss: -3.37\n",
      "Epoch: 0408 D loss: -0.4166 G loss: -3.307\n",
      "Epoch: 0409 D loss: -0.3314 G loss: -3.622\n",
      "Epoch: 0410 D loss: -0.3406 G loss: -3.504\n",
      "Epoch: 0411 D loss: -0.3935 G loss: -3.608\n",
      "Epoch: 0412 D loss: -0.42 G loss: -2.856\n",
      "Epoch: 0413 D loss: -0.3953 G loss: -3.448\n",
      "Epoch: 0414 D loss: -0.4062 G loss: -3.555\n",
      "Epoch: 0415 D loss: -0.3769 G loss: -3.328\n",
      "Epoch: 0416 D loss: nan G loss: nan\n",
      "Epoch: 0417 D loss: nan G loss: nan\n",
      "Epoch: 0418 D loss: nan G loss: nan\n",
      "Epoch: 0419 D loss: nan G loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dong7\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py:405: UserWarning: Warning: converting a masked element to nan.\n",
      "  dv = (np.float64(self.norm.vmax) -\n",
      "C:\\Users\\dong7\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py:406: UserWarning: Warning: converting a masked element to nan.\n",
      "  np.float64(self.norm.vmin))\n",
      "C:\\Users\\dong7\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py:412: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_min = np.float64(newmin)\n",
      "C:\\Users\\dong7\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py:417: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_max = np.float64(newmax)\n",
      "C:\\Users\\dong7\\Anaconda3\\lib\\site-packages\\matplotlib\\colors.py:916: UserWarning: Warning: converting a masked element to nan.\n",
      "  dtype = np.min_scalar_type(value)\n",
      "C:\\Users\\dong7\\Anaconda3\\lib\\site-packages\\numpy\\ma\\core.py:716: UserWarning: Warning: converting a masked element to nan.\n",
      "  data = np.array(a, copy=False, subok=subok)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0420 D loss: nan G loss: nan\n",
      "Epoch: 0421 D loss: nan G loss: nan\n",
      "Epoch: 0422 D loss: nan G loss: nan\n",
      "Epoch: 0423 D loss: nan G loss: nan\n",
      "Epoch: 0424 D loss: nan G loss: nan\n",
      "Epoch: 0425 D loss: nan G loss: nan\n",
      "Epoch: 0426 D loss: nan G loss: nan\n",
      "Epoch: 0427 D loss: nan G loss: nan\n",
      "Epoch: 0428 D loss: nan G loss: nan\n",
      "Epoch: 0429 D loss: nan G loss: nan\n",
      "Epoch: 0430 D loss: nan G loss: nan\n",
      "Epoch: 0431 D loss: nan G loss: nan\n",
      "Epoch: 0432 D loss: nan G loss: nan\n",
      "Epoch: 0433 D loss: nan G loss: nan\n",
      "Epoch: 0434 D loss: nan G loss: nan\n",
      "Epoch: 0435 D loss: nan G loss: nan\n",
      "Epoch: 0436 D loss: nan G loss: nan\n",
      "Epoch: 0437 D loss: nan G loss: nan\n",
      "Epoch: 0438 D loss: nan G loss: nan\n",
      "Epoch: 0439 D loss: nan G loss: nan\n",
      "Epoch: 0440 D loss: nan G loss: nan\n",
      "Epoch: 0441 D loss: nan G loss: nan\n",
      "Epoch: 0442 D loss: nan G loss: nan\n",
      "Epoch: 0443 D loss: nan G loss: nan\n",
      "Epoch: 0444 D loss: nan G loss: nan\n",
      "Epoch: 0445 D loss: nan G loss: nan\n",
      "Epoch: 0446 D loss: nan G loss: nan\n",
      "Epoch: 0447 D loss: nan G loss: nan\n",
      "Epoch: 0448 D loss: nan G loss: nan\n",
      "Epoch: 0449 D loss: nan G loss: nan\n",
      "Epoch: 0450 D loss: nan G loss: nan\n",
      "Epoch: 0451 D loss: nan G loss: nan\n",
      "Epoch: 0452 D loss: nan G loss: nan\n",
      "Epoch: 0453 D loss: nan G loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cff7b60b3c4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_val_D\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_D\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_val_G\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_G\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_G\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mZ\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     print('Epoch:','%04d' % epoch, 'D loss: {:.4}'.format(loss_val_D),\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\",one_hot=True)\n",
    "\n",
    "total_epoch = 100\n",
    "batch_size =100\n",
    "learning_rate = 0.0002\n",
    "n_hidden = 256\n",
    "n_input = 28*28\n",
    "n_noise = 128\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,n_input])\n",
    "Z = tf.placeholder(tf.float32,[None,n_noise])\n",
    "\n",
    "#generator\n",
    "G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden],stddev=0.01))\n",
    "G_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input],stddev=0.01))\n",
    "G_b2 = tf.Variable(tf.zeros([n_input]))\n",
    "\n",
    "#Discriminator\n",
    "D_W1 = tf.Variable(tf.random_normal([n_input, n_hidden],stddev=0.01))\n",
    "D_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "D_W2 = tf.Variable(tf.random_normal([n_hidden, 1],stddev=0.01))\n",
    "D_b2 = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "\n",
    "def generator(noise_z):\n",
    "    hidden = tf.nn.relu(tf.matmul(noise_z,G_W1)+G_b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden,G_W2)+G_b2)\n",
    "    return output\n",
    "\n",
    "def discriminator(inputs):\n",
    "    hidden = tf.nn.relu(tf.matmul(inputs,D_W1)+D_b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden,D_W2)+D_b2)\n",
    "    \n",
    "    return output\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.normal(size=(batch_size,n_noise))\n",
    "\n",
    "G = generator(Z)\n",
    "D_gene = discriminator(G)\n",
    "D_real = discriminator(X)\n",
    "\n",
    "#loss function\n",
    "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1-D_gene))\n",
    "loss_G = tf.reduce_mean(tf.log(D_gene))\n",
    "\n",
    "D_var_list = [D_W1, D_b1, D_W2, D_b2]\n",
    "G_var_list = [G_W1, G_b1, G_W2, G_b2]\n",
    "\n",
    "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list=D_var_list)\n",
    "train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G, var_list=G_var_list)\n",
    "\n",
    "#train\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "loss_val_D, loss_val_G =0,0\n",
    "\n",
    "for epoch in range(total_batch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "        \n",
    "        _,loss_val_D = sess.run([train_D, loss_D], feed_dict = {X:batch_xs, Z : noise})\n",
    "        _,loss_val_G = sess.run([train_G, loss_G], feed_dict = {Z : noise})\n",
    "        \n",
    "    print('Epoch:','%04d' % epoch, 'D loss: {:.4}'.format(loss_val_D),\n",
    "         'G loss: {:.4}'.format(loss_val_G))\n",
    "    if epoch == 0 or (epoch+1)% 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size,n_noise)\n",
    "        samples = sess.run(G, feed_dict={Z:noise})\n",
    "        \n",
    "        fig, ax = plt.subplots(1, sample_size, figsize=(sample_size, 1))\n",
    "        \n",
    "        for i in range(sample_size):\n",
    "            ax[i].set_axis_off()\n",
    "            ax[i].imshow(np.reshape(samples[i], (28,28)))\n",
    "            \n",
    "        plt.savefig('samples/{}.png'.format(str(epoch).zfill(3)),bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "print('finish')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dong7\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-9432bb607ef1>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\dong7\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\dong7\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\dong7\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\dong7\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\dong7\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Epoch: 0000 D loss: 0.01431 G loss: 7.833\n",
      "Epoch: 0001 D loss: 0.031 G loss: 5.945\n",
      "Epoch: 0002 D loss: 0.02658 G loss: 6.73\n",
      "Epoch: 0003 D loss: 0.003504 G loss: 8.183\n",
      "Epoch: 0004 D loss: 0.01425 G loss: 7.054\n",
      "Epoch: 0005 D loss: 0.01244 G loss: 8.99\n",
      "Epoch: 0006 D loss: 0.1685 G loss: 5.502\n",
      "Epoch: 0007 D loss: 0.07023 G loss: 6.525\n",
      "Epoch: 0008 D loss: 0.118 G loss: 7.415\n",
      "Epoch: 0009 D loss: 0.1179 G loss: 5.548\n",
      "Epoch: 0010 D loss: 0.2346 G loss: 6.616\n",
      "Epoch: 0011 D loss: 0.2624 G loss: 4.695\n",
      "Epoch: 0012 D loss: 0.2979 G loss: 5.374\n",
      "Epoch: 0013 D loss: 0.2769 G loss: 3.463\n",
      "Epoch: 0014 D loss: 0.2308 G loss: 4.765\n",
      "Epoch: 0015 D loss: 0.4754 G loss: 4.056\n",
      "Epoch: 0016 D loss: 0.3191 G loss: 4.103\n",
      "Epoch: 0017 D loss: 0.4298 G loss: 3.147\n",
      "Epoch: 0018 D loss: 0.607 G loss: 3.528\n",
      "Epoch: 0019 D loss: 0.6014 G loss: 2.842\n",
      "Epoch: 0020 D loss: 0.4335 G loss: 3.313\n",
      "Epoch: 0021 D loss: 0.4581 G loss: 3.309\n",
      "Epoch: 0022 D loss: 0.6138 G loss: 3.205\n",
      "Epoch: 0023 D loss: 0.7246 G loss: 3.12\n",
      "Epoch: 0024 D loss: 0.7877 G loss: 3.581\n",
      "Epoch: 0025 D loss: 0.8623 G loss: 2.45\n",
      "Epoch: 0026 D loss: 0.4987 G loss: 3.121\n",
      "Epoch: 0027 D loss: 0.6918 G loss: 2.511\n",
      "Epoch: 0028 D loss: 0.9186 G loss: 2.171\n",
      "Epoch: 0029 D loss: 0.7345 G loss: 2.946\n",
      "Epoch: 0030 D loss: 0.698 G loss: 2.763\n",
      "Epoch: 0031 D loss: 0.8186 G loss: 2.71\n",
      "Epoch: 0032 D loss: 0.5959 G loss: 2.683\n",
      "Epoch: 0033 D loss: 0.7154 G loss: 2.256\n",
      "Epoch: 0034 D loss: 0.568 G loss: 2.294\n",
      "Epoch: 0035 D loss: 0.7135 G loss: 2.624\n",
      "Epoch: 0036 D loss: 0.648 G loss: 2.02\n",
      "Epoch: 0037 D loss: 0.6587 G loss: 2.333\n",
      "Epoch: 0038 D loss: 0.6255 G loss: 2.355\n",
      "Epoch: 0039 D loss: 0.6621 G loss: 2.223\n",
      "Epoch: 0040 D loss: 0.8113 G loss: 2.278\n",
      "Epoch: 0041 D loss: 0.6549 G loss: 2.074\n",
      "Epoch: 0042 D loss: 0.7341 G loss: 2.095\n",
      "Epoch: 0043 D loss: 0.6932 G loss: 2.012\n",
      "Epoch: 0044 D loss: 0.7237 G loss: 2.428\n",
      "Epoch: 0045 D loss: 0.6461 G loss: 2.352\n",
      "Epoch: 0046 D loss: 0.6944 G loss: 2.378\n",
      "Epoch: 0047 D loss: 0.5995 G loss: 2.903\n",
      "Epoch: 0048 D loss: 0.6167 G loss: 2.395\n",
      "Epoch: 0049 D loss: 0.5232 G loss: 2.376\n",
      "Epoch: 0050 D loss: 0.6811 G loss: 1.941\n",
      "Epoch: 0051 D loss: 0.6216 G loss: 2.422\n",
      "Epoch: 0052 D loss: 0.7841 G loss: 2.071\n",
      "Epoch: 0053 D loss: 0.6983 G loss: 2.204\n",
      "Epoch: 0054 D loss: 0.6396 G loss: 2.598\n",
      "Epoch: 0055 D loss: 0.7022 G loss: 2.15\n",
      "Epoch: 0056 D loss: 0.7236 G loss: 2.349\n",
      "Epoch: 0057 D loss: 0.7695 G loss: 2.252\n",
      "Epoch: 0058 D loss: 0.6706 G loss: 2.568\n",
      "Epoch: 0059 D loss: 0.6661 G loss: 2.116\n",
      "Epoch: 0060 D loss: 0.6927 G loss: 2.332\n",
      "Epoch: 0061 D loss: 0.6173 G loss: 2.347\n",
      "Epoch: 0062 D loss: 0.7526 G loss: 2.358\n",
      "Epoch: 0063 D loss: 0.7043 G loss: 2.055\n",
      "Epoch: 0064 D loss: 0.5485 G loss: 2.314\n",
      "Epoch: 0065 D loss: 0.772 G loss: 2.244\n",
      "Epoch: 0066 D loss: 0.9262 G loss: 1.832\n",
      "Epoch: 0067 D loss: 0.6029 G loss: 2.28\n",
      "Epoch: 0068 D loss: 0.5982 G loss: 2.262\n",
      "Epoch: 0069 D loss: 0.6263 G loss: 2.576\n",
      "Epoch: 0070 D loss: 0.6772 G loss: 2.227\n",
      "Epoch: 0071 D loss: 0.6246 G loss: 2.406\n",
      "Epoch: 0072 D loss: 0.6505 G loss: 2.374\n",
      "Epoch: 0073 D loss: 0.6164 G loss: 2.308\n",
      "Epoch: 0074 D loss: 0.7666 G loss: 1.9\n",
      "Epoch: 0075 D loss: 0.8498 G loss: 1.906\n",
      "Epoch: 0076 D loss: 0.653 G loss: 2.064\n",
      "Epoch: 0077 D loss: 0.7208 G loss: 2.462\n",
      "Epoch: 0078 D loss: 0.5711 G loss: 2.268\n",
      "Epoch: 0079 D loss: 0.6827 G loss: 1.926\n",
      "Epoch: 0080 D loss: 0.6742 G loss: 1.949\n",
      "Epoch: 0081 D loss: 0.802 G loss: 2.285\n",
      "Epoch: 0082 D loss: 0.6768 G loss: 2.112\n",
      "Epoch: 0083 D loss: 0.6412 G loss: 2.685\n",
      "Epoch: 0084 D loss: 0.9358 G loss: 1.793\n",
      "Epoch: 0085 D loss: 0.744 G loss: 1.998\n",
      "Epoch: 0086 D loss: 0.5901 G loss: 2.256\n",
      "Epoch: 0087 D loss: 0.5986 G loss: 2.463\n",
      "Epoch: 0088 D loss: 0.6897 G loss: 2.164\n",
      "Epoch: 0089 D loss: 0.827 G loss: 1.908\n",
      "Epoch: 0090 D loss: 0.6907 G loss: 2.147\n",
      "Epoch: 0091 D loss: 0.673 G loss: 1.924\n",
      "Epoch: 0092 D loss: 0.7397 G loss: 2.08\n",
      "Epoch: 0093 D loss: 0.7519 G loss: 2.071\n",
      "Epoch: 0094 D loss: 0.6082 G loss: 2.247\n",
      "Epoch: 0095 D loss: 0.665 G loss: 2.223\n",
      "Epoch: 0096 D loss: 0.8153 G loss: 1.807\n",
      "Epoch: 0097 D loss: 0.5912 G loss: 1.91\n",
      "Epoch: 0098 D loss: 0.7738 G loss: 1.888\n",
      "Epoch: 0099 D loss: 0.8749 G loss: 1.791\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\",one_hot=True)\n",
    "\n",
    "total_epoch = 100\n",
    "batch_size =100\n",
    "n_hidden = 256\n",
    "n_input = 28*28\n",
    "n_noise = 128\n",
    "n_class = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,n_input])\n",
    "Z = tf.placeholder(tf.float32,[None,n_noise])\n",
    "Y = tf.placeholder(tf.float32,[None,n_class])\n",
    "\n",
    "def generator(noise, labels):\n",
    "    with tf.variable_scope('generator'):\n",
    "        inputs = tf.concat([noise,labels],1)\n",
    "        hidden = tf.layers.dense(inputs, n_hidden, activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden,n_input,activation=tf.nn.sigmoid)\n",
    "    return output\n",
    "def discriminator(inputs, labels, reuse=None):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        inputs = tf.concat([inputs,labels],1)\n",
    "        hidden = tf.layers.dense(inputs, n_hidden, activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden, 1, activation=None)\n",
    "    return output\n",
    "\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.uniform(-1.,1.,size=[batch_size,n_noise])\n",
    "\n",
    "G = generator(Z, Y)\n",
    "D_real = discriminator(X,Y)\n",
    "D_gene = discriminator(G,Y,True)\n",
    "\n",
    "loss_D_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real,labels=tf.ones_like(D_real)))\n",
    "loss_D_gene = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_gene,labels=tf.zeros_like(D_gene)))\n",
    "\n",
    "loss_D = loss_D_real+loss_D_gene\n",
    "\n",
    "loss_G = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_gene,labels=tf.ones_like(D_gene)))\n",
    "\n",
    "vars_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "vars_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "train_D = tf.train.AdamOptimizer().minimize(loss_D, var_list=vars_D)\n",
    "train_G = tf.train.AdamOptimizer().minimize(loss_G, var_list = vars_G)\n",
    "\n",
    "#train\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples/ batch_size)\n",
    "loss_val_D, loss_val_G = 0,0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "\n",
    "        _, loss_val_D = sess.run([train_D, loss_D],\n",
    "                                 feed_dict={X: batch_xs, Y: batch_ys, Z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G],\n",
    "                                 feed_dict={Y: batch_ys, Z: noise})\n",
    "\n",
    "    print('Epoch:', '%04d' % epoch,\n",
    "          'D loss: {:.4}'.format(loss_val_D),\n",
    "          'G loss: {:.4}'.format(loss_val_G))\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G,\n",
    "                           feed_dict={Y: mnist.test.labels[:sample_size],\n",
    "                                      Z: noise})\n",
    "\n",
    "        fig, ax = plt.subplots(2, sample_size, figsize=(sample_size, 2))\n",
    "\n",
    "        for i in range(sample_size):\n",
    "            ax[0][i].set_axis_off()\n",
    "            ax[1][i].set_axis_off()\n",
    "\n",
    "            ax[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n",
    "            ax[1][i].imshow(np.reshape(samples[i], (28, 28)))\n",
    "\n",
    "        plt.savefig('samples/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "print('최적화 완료!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
